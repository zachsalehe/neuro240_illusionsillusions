{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e9853c3-f0e0-4f15-9f0e-0b381fe4369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/spack/opt/spack/linux-amzn2-skylake_avx512/gcc-14.1.0/miniconda3-24.3.0-zxx5jostrj4myhf7bi3oap3ylkmegd3a/envs/neuro140/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-28 23:27:23.602957: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-28 23:27:23.602996: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-28 23:27:23.603922: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-28 23:27:23.609421: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-28 23:27:28.053179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "from ncut_pytorch import NCUT, rgb_from_tsne_3d\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoModel, AutoProcessor, CLIPTokenizer, CLIPTextModelWithProjection\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import requests\n",
    "from PIL import Image, ImageOps\n",
    "import accelerate\n",
    "import gc\n",
    "from diffusers import StableDiffusion3Pipeline, AutoencoderKL, SD3Transformer2DModel\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72a72563-bf20-4622-904d-ef79f2161184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\") # login token removed since this is going on a public repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27b3a6fd-ec78-4ec3-91d8-ae9b3684cd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, size=(448, 448), pad=(255, 255, 255)):\n",
    "    image.thumbnail((size[0], size[1]), Image.Resampling.LANCZOS)\n",
    "\n",
    "    resized = Image.new(\"RGB\", size, pad)\n",
    "\n",
    "    x_offset = (size[0] - image.size[0]) // 2\n",
    "    y_offset = (size[1] - image.size[1]) // 2\n",
    "\n",
    "    resized.paste(image, (x_offset, y_offset))\n",
    "\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be728c8d-2a96-4209-a92e-46c4d245d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_paligemma_features(images):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    model_id = \"google/paligemma2-3b-ft-docci-448\"\n",
    "    model = PaliGemmaForConditionalGeneration.from_pretrained(model_id, attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16, device_map=\"auto\").to(\"cuda\").eval()\n",
    "    processor = PaliGemmaProcessor.from_pretrained(model_id)\n",
    "\n",
    "    processor.do_resize = False\n",
    "    processor.do_center_crop = False \n",
    "\n",
    "    text = [\"<image>\" for i in range(len(images))]\n",
    "    model_inputs = processor(text=text, images=images, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vision_outputs = model.vision_tower.vision_model(pixel_values=model_inputs[\"pixel_values\"])\n",
    "        features = vision_outputs.last_hidden_state.to(torch.float32)\n",
    "\n",
    "    return features.reshape(len(images), 32, 32, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb54fe11-b2f8-4e45-b1b0-b7638a9b4ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_qwen_features(images):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "    model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(model_id, attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16, device_map=\"auto\").to(\"cuda\").eval()\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    processor.do_resize = False\n",
    "    processor.do_center_crop = False \n",
    "\n",
    "    model_inputs = processor(text=\"\", images=images, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vision_outputs = model.visual(hidden_states=model_inputs[\"pixel_values\"], grid_thw=model_inputs[\"image_grid_thw\"])\n",
    "        features = vision_outputs.to(torch.float32)\n",
    "        \n",
    "    return features.reshape(len(images), 32, 32, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df31eda8-8750-4467-aeef-7f98dd4ada13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dino_features(images):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    model_id = \"facebook/dinov2-base\"\n",
    "    model = AutoModel.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\").to(\"cuda\").eval()\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    processor.do_resize = False\n",
    "    processor.do_center_crop = False \n",
    "\n",
    "    model_inputs = processor(images=images, return_tensors=\"pt\").to(torch.bfloat16).to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vision_outputs = model(**model_inputs)\n",
    "        features = vision_outputs.last_hidden_state.to(torch.float32)[:,1:]\n",
    "\n",
    "    return features.reshape(len(images), 32, 32, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cca19f6f-82cf-411b-abf3-28f68a1ade59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stable_diffusion_features(images):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    \n",
    "    model_id = \"stabilityai/stable-diffusion-3.5-medium\"\n",
    "    model_vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "    model_transformer = SD3Transformer2DModel.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16).to(\"cuda\")\n",
    "\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "    \n",
    "    image_tensors = []\n",
    "    for image in images:\n",
    "        image_tensor = transform(image).to(torch.bfloat16).to(model_vae.device)\n",
    "        image_tensors.append(image_tensor)\n",
    "\n",
    "    image_tensors = torch.stack(image_tensors, dim=0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        latent_distributions = model_vae.encode(image_tensors)\n",
    "        latents = latent_distributions.latent_dist.sample()\n",
    "\n",
    "    vision_outputs = []\n",
    "    def save_vision_outputs(module, input, output):\n",
    "        vision_outputs.append(output)\n",
    "\n",
    "    model_transformer.transformer_blocks[22].register_forward_hook(save_vision_outputs)\n",
    "\n",
    "    timestep = torch.tensor([0], dtype=torch.long, device=latents.device)\n",
    "    pooled_projections = torch.zeros((latents.shape[0], 2048), dtype=latents.dtype, device=latents.device)\n",
    "    text_embeddings = torch.zeros((latents.shape[0], 77, 4096), dtype=latents.dtype, device=latents.device)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        model_transformer(hidden_states=latents, timestep=timestep, pooled_projections=pooled_projections, encoder_hidden_states=text_embeddings,)\n",
    "\n",
    "    features = vision_outputs[0][1]\n",
    "\n",
    "    return features.reshape(len(images), 32, 32, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "978275a6-9d02-4e98-90f0-a03e8d25a086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:13<00:00,  6.89s/it]\n"
     ]
    }
   ],
   "source": [
    "image_files = sorted(glob.glob(\"data/*_base.png\") + glob.glob(\"data/*_test.png\"))\n",
    "\n",
    "images = []\n",
    "for image_file in image_files:\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    image = resize(image, size=(448, 448))\n",
    "    images.append(image)\n",
    "\n",
    "paligemma_features = compute_paligemma_features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8be615a-a88c-4a97-8c1b-0083dce66f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:16<00:00,  8.40s/it]\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "image_files = sorted(glob.glob(\"data/*_base.png\") + glob.glob(\"data/*_test.png\"))\n",
    "\n",
    "images = []\n",
    "for image_file in image_files:\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    image = resize(image, size=(896, 896))\n",
    "    images.append(image)\n",
    "\n",
    "qwen_features = compute_qwen_features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf2d46d3-c517-42c2-9074-f8da90ce2ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted(glob.glob(\"data/*_base.png\") + glob.glob(\"data/*_test.png\"))\n",
    "\n",
    "images = []\n",
    "for image_file in image_files:\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    image = resize(image, size=(448, 448))\n",
    "    images.append(image)\n",
    "\n",
    "dino_features = compute_dino_features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd4cf9a0-6db2-48bc-840a-5e883197fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted(glob.glob(\"data/*_base.png\") + glob.glob(\"data/*_test.png\"))\n",
    "\n",
    "images = []\n",
    "for image_file in image_files:\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    image = resize(image, size=(512, 512))\n",
    "    images.append(image)\n",
    "\n",
    "stable_diffusion_features = compute_stable_diffusion_features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1644a6a1-8d40-4ceb-b847-a6cbe9736cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = {\"paligemma_features\": paligemma_features,\n",
    "                \"qwen_features\": qwen_features,\n",
    "                \"dino_features\": dino_features,\n",
    "                \"stable_diffusion_features\": stable_diffusion_features}\n",
    "\n",
    "torch.save(all_features, \"all_features.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
