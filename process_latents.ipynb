{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7124952-688d-4dee-bd2a-20b40e187d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/shared/spack/opt/spack/linux-amzn2-skylake_avx512/gcc-14.1.0/miniconda3-24.3.0-zxx5jostrj4myhf7bi3oap3ylkmegd3a/envs/neuro140/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-04-14 14:42:19.970072: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-14 14:42:19.970117: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-14 14:42:19.988090: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-14 14:42:20.041630: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-14 14:42:26.792122: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "from ncut_pytorch import NCUT, rgb_from_tsne_3d\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import PaliGemmaProcessor, PaliGemmaForConditionalGeneration, Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoModel, AutoProcessor, CLIPTokenizer, CLIPTextModelWithProjection\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import requests\n",
    "from PIL import Image, ImageOps\n",
    "import accelerate\n",
    "import gc\n",
    "from diffusers import StableDiffusion3Pipeline, AutoencoderKL, SD3Transformer2DModel\n",
    "import functools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8636d223-69f0-4ebf-821e-ba12ee37087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(\"\") # login token removed since this is going on a public repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efc0a399-06d9-4add-9180-8718d33a4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, size=(448, 448), pad=(255, 255, 255)):\n",
    "    image.thumbnail((size[0], size[1]), Image.Resampling.LANCZOS)\n",
    "\n",
    "    resized = Image.new(\"RGB\", size, pad)\n",
    "\n",
    "    x_offset = (size[0] - image.size[0]) // 2\n",
    "    y_offset = (size[1] - image.size[1]) // 2\n",
    "\n",
    "    resized.paste(image, (x_offset, y_offset))\n",
    "\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b70075-784d-4ceb-b97d-d03f3e8ddc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted(glob.glob(\"data/*_base.png\") + glob.glob(\"data/*_test.png\"))\n",
    "\n",
    "images = []\n",
    "for image_file in image_files:\n",
    "    image = Image.open(image_file).convert(\"RGB\")\n",
    "    image = resize(image, size=(512, 512))\n",
    "    images.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc76214-1f1b-4062-800a-befd89639532",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.ipc_collect()\n",
    "\n",
    "model_id = \"stabilityai/stable-diffusion-3.5-medium\"\n",
    "model_vae = AutoencoderKL.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.bfloat16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358c9300-185d-4a89-ba49-5101498119b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "image_tensors = []\n",
    "for image in images:\n",
    "    image_tensor = transform(image).to(torch.bfloat16).to(model_vae.device)\n",
    "    image_tensors.append(image_tensor)\n",
    "\n",
    "image_tensors = torch.stack(image_tensors, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37660252-7aaf-4b30-abd7-235ec8ae736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    latent_distributions = model_vae.encode(image_tensors)\n",
    "    latents = latent_distributions.latent_dist.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a480e7d6-92f5-4f9e-85d3-d31451216ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_similarity(feat1, feat2):    \n",
    "    sims1 = torch.zeros((feat1.shape[0]), device=feat1.device)\n",
    "    for i in range(feat1.shape[0]):\n",
    "        sims1[i] = torch.clip(F.cosine_similarity(feat1[i:i+1], feat2, dim=1).max(), -1, 1)\n",
    "\n",
    "    sims2 = torch.zeros((feat2.shape[0]), device=feat2.device)\n",
    "    for i in range(feat2.shape[0]):\n",
    "        sims2[i] = torch.clip(F.cosine_similarity(feat2[i:i+1], feat1, dim=1).max(), -1, 1)\n",
    "\n",
    "    return ((sims1.mean() + sims2.mean()) / 2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f852c9a1-699e-4ce9-8a45-41bf114cdf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = np.zeros((latents.shape[0], latents.shape[0]))\n",
    "\n",
    "for i in range(latents.shape[0]):\n",
    "    lat1 = torch.permute(latents[i], (1, 2, 0))\n",
    "    lat1 = lat1.reshape(-1, lat1.shape[-1]).to(\"cuda:0\")\n",
    "    \n",
    "    for j in range(latents.shape[0]):\n",
    "        lat2 = torch.permute(latents[j], (1, 2, 0))\n",
    "        lat2 = lat2.reshape(-1, lat2.shape[-1]).to(\"cuda:0\")\n",
    "        \n",
    "        sims[i,j] = nn_similarity(lat1, lat2)\n",
    "\n",
    "np.save(\"latent_similarities.npy\", sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c0b8b9-7701-425b-afa7-f33c2754edc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = np.load(\"latent_similarities.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ee0f205-d7d2-421b-84fe-3ba0f8568664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latents illusion / illusion-illusion: 0.9943636894226074 [0.9960331916809082, 0.9732928276062012, 0.9966030120849609, 0.9925751686096191, 0.9979376792907715, 0.9969453811645508, 0.9980902671813965, 0.9995484352111816, 0.9933309555053711, 0.9992799758911133]\n",
      "latents illusion / other illusion: 0.9530617746710777 [0.9669348955154419, 0.8939115881919861, 0.9482133358716964, 0.9688911437988281, 0.9725773841142654, 0.9645303130149842, 0.9393180638551712, 0.9479171335697174, 0.9534059286117553, 0.9749179601669311]\n"
     ]
    }
   ],
   "source": [
    "illusionillusion_sim = []\n",
    "otherillusion_sim = []\n",
    "\n",
    "for i in range(0, 20, 2):\n",
    "    illusionillusion_sim.append(sims[i][i+1])\n",
    "\n",
    "    idx = list(range(0, 20, 2)).remove(i)\n",
    "    otherillusion_sim.append(sims[i][idx].mean())\n",
    "\n",
    "print(\"latents illusion / illusion-illusion:\", np.mean(illusionillusion_sim), illusionillusion_sim)\n",
    "print(\"latents illusion / other illusion:\", np.mean(otherillusion_sim), otherillusion_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
